\documentclass[a4paper, 12pt, notitlepage]{report}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage[margin=3cm]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subcaption}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\title{\textbf{Making the Cloud Efficient: In-depth study of cloud computing and Resource Management}}
\date{}
\begin{document}

\maketitle
\begin{center}
\Large
\textbf{CS677 - Operating Systems}\\
\vspace{350pt}
\textbf{Submitted by}:\\
Vijay Pasikanti, Sneha Shankar Narayan\\
\end{center}
\newpage

\tableofcontents

\chapter{Introduction}
Cloud computing platforms are complex systems with many VMs which
in turn run different applications. Since applications have varying
needs, different VMs are configured differently. The goal of this
project is to write a simple tool that is inspired by Amazon Trusted
Advisor that is used in Amazon's EC2 cloud to continuously monitor a
cloud deployment and make recommendations on how to improve security,
resource usage, etc for various VMs by analyzing the hypervisor. The
project involves setting up OpenStack on two nodes, setting up
hypervisors and virtual machines and then writing a tool that uses the
OpenStack API as well as the hypervisor API to gather various
statistics that are then analyzed to provide recommendations.

In this project we focus on monitoring the statistics based on which we provide resource utilization recommendations.The hypervisor that is being used here is KVM.

\chapter{Openstack setup}
Openstack\cite{openstack} is a cloud management platform where in there is a controller node and a compute node.
\begin{itemize}
\item The controller node runs the Identity Service, Image Service, dashboard, and management portion of Compute. It also contains the associated API services, MySQL databases, and messaging system.
\item The compute node runs the hypervisor portion of Compute, which operates tenant virtual machines. By default, Compute uses KVM as the hypervisor. Compute also provisions and operates tenant networks and implements security groups. You can run more than one compute node.
\end{itemize}

We have one controller node and one compute node and the following open stack services have been installed
\begin{itemize}
\item \textbf{Identity service (Keystone):} Keystone is responsible for user management and service catalog.
\item \textbf{Image service (Glance):} Glance is responsible for  image discovery, retrieval, and storage, storing and retrieving metadata about images and for managing a storage repository for images.
\item \textbf{Compute service (Nova):} Nova is a cloud computing fabric controller. It is used to host and manage cloud computing systems. Compute interacts with the Identity Service for authentication, Image Service for images, and the Dashboard for the user and administrative interface. Access to images is limited by project and by user; quotas are limited per project (for example, the number of instances). The Compute service scales horizontally on standard hardware, and downloads images to launch instances as required.
\end{itemize}

The messaging between the various open stack services is done using RabbitMQ.

Our open stack architecture is configured the following way:
\begin{center}
\includegraphics[scale = 0.75]{basic-architecture}
\end{center}

After configuring the controller and the compute node a network was setup between them. We have launched various instances of Ubuntu on our compute node. Currently we having the following VMs running in our compute node
\scriptsize
\begin{verbatim}

root@obelix24:~# nova list
+--------------------------------------+----------+--------+------------+-------------+-----------------+
| ID                                   | Name     | Status | Task State | Power State | Networks        |
+--------------------------------------+----------+--------+------------+-------------+-----------------+
| d7cc815a-e158-4253-b4e4-65b27615fb86 | ubuntu11 | ACTIVE | None       | Running     | vmnet=10.0.0.2  |
| d53d43ba-27e0-4b45-afe0-ca566704953a | ubuntu12 | ACTIVE | None       | Running     | vmnet=10.0.0.3  |
| 0c53289c-a089-4f89-96f1-efff70fcbd9a | ubuntu13 | ACTIVE | None       | Running     | vmnet=10.0.0.4  |
| 1a1678fb-8daa-4647-89a7-414312be46d5 | ubuntu14 | ACTIVE | None       | Running     | vmnet=10.0.0.5  |
| 33251435-11f3-4013-ad30-4b4c94d99188 | ubuntu15 | ACTIVE | None       | Running     | vmnet=10.0.0.6  |
| 2b108650-c32e-4695-ae04-00622e61d8f4 | ubuntu16 | ACTIVE | None       | Running     | vmnet=10.0.0.7  |
| 56e157c6-a06f-4a6b-b152-b58664ad0c5e | ubuntu17 | ACTIVE | None       | Running     | vmnet=10.0.0.8  |
| eb54b3e2-5c5d-4c7b-a1ec-297fa96c53e9 | ubuntu18 | ACTIVE | None       | Running     | vmnet=10.0.0.9  |
| 64311806-d774-4f3a-b2bb-aacf4e6f9360 | ubuntu19 | ACTIVE | None       | Running     | vmnet=10.0.0.10 |
+--------------------------------------+----------+--------+------------+-------------+-----------------+

\end{verbatim}

\normalsize

The instances are managed by the nova compute service.

\chapter{Existing monitoring systems explored}
	\section{AWS Trusted Advisor}
		AWS trusted advisor \cite{aws} is a monitoring tool for Amazon Web Services (AWS), it inspects AWS environment and makes recommendations when opportunities exist to optimize costs, improve system performance, reliability and security. It provides best practices in four categories: cost optimization, security, fault tolerance and performance improvement. It covers almost all the AWS services. 
		\\\\
		\textbf{Advantages}:
		\begin{itemize}
			\item
			Provides a wide range of recommendations in almost all the AWS services.
			\item
			Developed and supported by Amazon for AWS users with new features being added continuously.
		\end{itemize}
		\textbf{Limitations}:
		\begin{itemize}
			\item
			Proprietary software and it's premium services are charged. 
			\item
			Doesn't support open source cloud platforms, for example: Openstack.
		\end{itemize}

	\section{Collectd - libvirt plugin}
		Libvirt-plugin \cite{collectd} is a part of \textit{collectd} which uses the virtualization API libvirt. It gathers statistics about virtualized guests on a system. It collects CPU, network interface and block device usages for each guest without installing \textit{collectd} on the guest systems. \\\\
		\textbf{Advantages}:
		\begin{itemize}
			\item
			Statistics are received from the hypervisor directly, this works not only with para-virtualized hosts but with hardware virtualized machines also.
			\item
			Eliminates the need of installing a monitoring tool within each virtual machine (VM). One instance of \textit{collectd} per hypervisor is enough to monitor all the VMs.
			\item
			Uses \textit{libvirt} library which provides support for more then one virtualization techniques. Currently supports Xen, Qemu and KVM backends.
		\end{itemize}
		\textbf{Limitations}:
		\begin{itemize}
			\item
			Data collection is automatic process but monitoring and provisioning of resources is still a manual process. 
		\end{itemize}
		
	\section{Ganglia}
	Ganglia\cite{ganglia} is a scalable distributed monitoring tool for high-performance systems such as clusters and grids. It provides system level historical statistics for all machines that are being monitored. The ganglia system comprises of two unique daemons: Ganglia Monitoring Daemon (\textit{gmond}) and Ganglia Meta Daemon (\textit{gmetad}). 
	\\\\
	\textbf{\textit{gmond}}:  \textit{gmond} runs on each node that is monitored. It collects system level parameters (e.g.CPU, Network Utilization) and transmits the collected statistics to \textit{gmetad} in XDR or XML format. \\\\
	\textbf{\textit{gmetad}}: \textit{gmetad} runs on one of the nodes of a cluster. It receives information by periodically polls a collection of data sources (defined in its configuration, usually all the nodes in the cluster it is a part of), parses the collected XML, saves all numeric, volatile metrics in round-robin databases (RRD) and exports the aggregate XML to clients. Data sources may be either gmond daemons, representing specific clusters, or other gmetad demons, representing sets of clusters.  \\
	\\
	\textbf{Advantages}:
	\begin{itemize}
		\item
		Since it is based on a hierarchical design targeted at federation of clusters, it can scale to any size of clusters or grids.
		\item
		It uses operating system calls for collecting different metrics. So, it can be used to monitor virtual machines by installing \textit{gmond} on VMs.
	\end{itemize}
	\textbf{Limitations}:
	\begin{itemize}
		\item
		When used with hypervisor to monitor virtual machines, it requires \textit{gmond} to be run on each of the virtual machines. In production deployments \textit{admin} has no control over the applications installed on the VMs. It is hard to enforce the installation of \textit{gmond} on each VM.
	\end{itemize}

\chapter{Openstack monitor}
The Openstack monitor that we have implemented is written completely in python, it has various modules to collect the statistics from the different virtual machines, store the statistics on disk, retrieve the stored data and provide recommendations on a web interface. 
\section{Performance metrics}
The performance metrics that we have decided to use to provide resource utilization specific metrics are:
\begin{itemize}
\item Average CPU utilization
\item Average memory utilization
\item Average disk utilization
\item Average network utilization
\item Average disk input/output
\end{itemize}	

Our aim was to analyze the above metrics and warn the user when say, the CPU utilization becomes greater than 90 \% which in turn means he needs to move the VM elsewhere or he can increase the resource allocated to the VM.

\section{Design and Implementation}
On a high level there is daemon which runs every five seconds to query the statistics from the hypervisor and this data is stored in Round Robin databases\cite{rrdindex}. There is another daemon which reads the data from the round robin files, provides recommendations and stores it as JSON files which is shown on the web interface which is updated every minute. 
	
\subsection{Collection of statistics}
The various statistics that are required by our system to provide recommendations was collected using libvirt APIs. The following libvirt modules were used
\begin{itemize}
\item \textbf{virt-top:} virt-top \cite{virttop} is similar to the unix functionality 'top' which provides data regarding the CPU, network and memory utilization of the processes in the system. Virt-top however looks at hypervisor level information and provides data that is specific to the virtual instances in the machine. The output of virt-top was streamed and statistics about the CPU, memory, disk-IO and network utilization of the VMs was collected.
\item \textbf{virt-df:} Virt-df \cite{virtdf}is similar to the unix functionality 'df' which lists the disks that are mounted and the used and available space in a disk. Virt-df looks into the various virtual machines and reports the storage space that is in use and that is available. Thus, disk utilization was collected.
\end{itemize}

A daemon runs every five seconds and captures the data from the above libvirt modules and stores the data in Round robin database files.

\subsection{Storing in RRD files}
The main motivation behind using Round robin databases was because the RRDtool can manage time-series data effectively, won't grow infinitely in size because of the round robin nature of the database and was also easily integrable with python. RRDtool can be configured to calculate the rate of change from the previous to the current value and store this information instead.

Using the python API of RRD tool\cite{rrddoc} the daemon that collects the various statistics also updates the RRD files associated with each of the metrics collected of each instance. 

Samples of the metrics collected have been stored for the following time intervals
\begin{itemize}
\item 5 second averages for 1 day
\item 30min averages for 5 days
\item 1 hour averages for 10 days
\item 2 hour averages for 20 days
\item 12 hour averages for 20 days
\item 12 hour averages for 20 days
\item 1 week averages for 10 weeks
\end{itemize}

Also we store average maximum and minumum values for memory utilization as well for the following time intervals
\begin{itemize}
\item 5 second averages for 1 day
\item 1 hour averages for 5 days
\item 1 day averages for 10 days
\end{itemize}

\subsection{Processing statistics and giving recommendations}
Another daemon has been configured to run indefinitely and it reads the statistics from the RRD files, processes them and provides recommendations. Recommendations are given based on the thresholds defined and we have defined the following thresholds.

\begin{itemize}
\item Average CPU utilization
\begin{itemize}
\item High: 80\%, to recommend user to upgrade hardware
\item Low: 20\%, to alert user about resource under utilization
\end{itemize}
\item Average memory utilization
\begin{itemize}
\item High: 90\%, to recommend user to upgrade hardware
\item Low: 20\%, to alert user about resource under utilization
\end{itemize}
\item Average disk utilization
\begin{itemize}
\item High: 75\%, to recommend user to add more disks
\end{itemize}
\item Average network utilization
\begin{itemize}
\item High: 10000 bytes transferred, to recommend user to increase bandwidth allocation
\item Low: 10 bytes transferred, to recommend user to downgrade bandwidth for instance
\end{itemize}
\item Average disk input/output
\begin{itemize}
\item High: 10000 IOps done, Alert user about high I/O
\item Low: 50 IOps done, alert user about resource under utilization
\end{itemize}
\end{itemize}

Using the values stored in the RRD files if it is found that any of these thresholds have been hit in the past hour that fact is recorded. This daemon spits out such recommendations to JSON files which is in turn fed into the web interface when it gets updated. 

\subsection{Web interface}
A simple python HTTP server is currently running in the compute node of the openstack setup and by using SSH tunneling we can look at the results on any browser. The webpage shows the recommendations provided by the Openstack monitor. The JSON files dumped by the daemon that processes the statistics is read using AJAX and the webpage is dynamically updated every minute with the latest data. The critical alerts are highlighted in red, low utilization and shown in blue such that a user who uses the monitoring system can take an appropriate action with respect to the configuration of the virtual machines. 

\chapter{Testing}
	\section{Setup}
	We used the same setup that was installed as described in the "Openstack Setup" section.
	\\
	Setup details:
	\begin{enumerate}
		\item
		Total hosts used: 2 (1 compute node and 1 controller node)
		\item
		Virtual machines: 9 VMs with Ubuntu 12.04 created with flavor \textit{m1.small} (1 VCPU, 2GB memory)
	\end{enumerate}
	\section{Test cases} 
	Random loads are generated in all virtual machines using lookbusy \cite{lookbusy}, a synthetic load generator for linux systems. The following are the scenarios that we simulate:
	\begin{itemize}
	\item CPU stress: CPU is loaded from 10\% to 90\% randomly for upto 2 hours.
	\item Memory stress: The VM memory is filled upto 1.4GB randomly for upto 2 hours.
	\item Disk I/O stress: The VM disk is churned with file sizes upto 32MB randomly for upto 2 hours.
	\end{itemize}
		
		
\chapter{Results} 
The recommendations are shown in the web interface as follows
\begin{figure}[H]
\centering
\fbox{\includegraphics[scale=0.32]{Screenshot}}	
\caption{Recommendation web interface}
\end{figure}

\textbf{Console output:}
\fontsize{10}{1.5}
\begin{verbatim}
Timestamp: 2014-05-05 18:52:15
instance-00000024 High cpu utilization (Average for the past 1 hour: 86.12%).
instance-00000024 Low network utilization (Average transfered bytes for the past 1 hour: 0.0).
instance-00000022 High cpu utilization (Average for the past 1 hour: 89.94%).
instance-00000022 Low network utilization (Average transfered bytes for the past 1 hour: 0.0).
instance-00000021 CPUs under utilized (Average for the past 1 hour: 0.0%).
instance-00000021 Low network utilization (Average transfered bytes for the past 1 hour: 0.0).
instance-00000028 High cpu utilization (Average for the past 1 hour: 88.98%).
instance-00000028 Low network utilization (Average transfered bytes for the past 1 hour: 0.0).
instance-00000027 CPUs under utilized (Average for the past 1 hour: 0.0%).
instance-00000027 Low network utilization (Average transfered bytes for the past 1 hour: 0.0).
instance-00000023 High cpu utilization (Average for the past 1 hour: 89.88%).
instance-00000023 Low network utilization (Average transfered bytes for the past 1 hour: 0.0).
instance-00000026 High cpu utilization (Average for the past 1 hour: 90.01%).
instance-00000026 Low network utilization (Average transfered bytes for the past 1 hour: 0.0).
\end{verbatim}

\normalsize
The below graphs show that the CPU utilization is constantly high for instances 22 and 24 which is why we see them in the list of recommendations shown above. 


\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{instance22-cpu}	
\caption{High CPU utilization on instance-00000022}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{instance24-cpu}	
\caption{High CPU utilization on instance-00000024}
\end{figure}

The CPU utilization is almost negligible for instance 27 which is captured below.
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{instance27-low-cpu}	
\caption{Low CPU utilization on instance-00000027}
\end{figure}

\chapter{Conclusion}
We were able to read the various statistics like CPU utilization, memory utilization, network bandwidth utilization, and disk utilization of the virtual machines in the openstack compute node by querying the KVM hypervisor using libvirt APIs. We were able to process the data collected and provide recommendations to the user/open stack administrator to take appropriate actions regarding the virtual machine configuration.

This monitoring system can be extended to record various other statistics of the virtual machines in order to build a full fledged open source equivalent of the Amazon Trusted Advisor. 

\begin{thebibliography}{}
\bibitem{openstack} \url{http://docs.openstack.org/trunk/install-guide/install/apt/content/}
\bibitem{aws} \url{https://aws.amazon.com/premiumsupport/trustedadvisor/}
\bibitem{rrdindex} \url{http://oss.oetiker.ch/rrdtool/index.en.html}
\bibitem{rrddoc} \url{http://oss.oetiker.ch/rrdtool/doc/index.en.html}
\bibitem{rrdtut} \url{http://oss.oetiker.ch/rrdtool/tut/rrdtutorial.en.html}

\bibitem{collectd} \url{http://collectd.org/}
\bibitem{ganglia} \url{http://ganglia.sourceforge.net/} 
\bibitem{nagios} \url{http://www.nagios.org/}
\bibitem{sensu} \url{http://sensuapp.org/}
\bibitem{lookbusy} \url{http://www.devin.com/lookbusy/}
\bibitem{libvirt} \url{http://libvirt.org/}
\bibitem{libvirtPython} \url{http://libvirt.org/python.html}
\bibitem{virsh} \url{http://libvirt.org/virshcmdref.html}
\bibitem{virttop} \url{http://people.redhat.com/~rjones/virt-top/}
\bibitem{virtdf} \url{http://libguestfs.org/virt-df.1.html}
\end{thebibliography}
\end{document}